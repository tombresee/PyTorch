{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ColabGPUguide.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tombresee/PyTorch/blob/main/ColabGPUguide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrvvwZUGE4b5"
      },
      "source": [
        "# Colab GPU Acceleration Setup Guide\n",
        "\n",
        "**First, make a copy of this notebook for yourself so that you can make edits. \"File\" -> \"Save a copy in Drive\"**\n",
        "\n",
        "Above select the \"Runtime\" dropdown -> \"Change runtime type\". You should use Python 3, and \"hardware accelerator\" should be \"GPU\".\n",
        "\n",
        "The following code cell will ensure that you have the GPU enabled. It will also provide system specifications. You should see something similar (with possibly different specs) as:\n",
        "\n",
        "```\n",
        "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-d977e794-801c-3a65-2cd2-2fe83043d501)\n",
        "Wed Apr  8 23:17:24 2020       \n",
        "+-----------------------------------------------------------------------------+\n",
        "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
        "|-------------------------------+----------------------+----------------------+\n",
        "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
        "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
        "|===============================+======================+======================|\n",
        "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
        "| N/A   33C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
        "+-------------------------------+----------------------+----------------------+\n",
        "                                                                               \n",
        "+-----------------------------------------------------------------------------+\n",
        "| Processes:                                                       GPU Memory |\n",
        "|  GPU       PID   Type   Process name                             Usage      |\n",
        "|=============================================================================|\n",
        "|  No running processes found                                                 |\n",
        "+-----------------------------------------------------------------------------+\n",
        "Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n",
        "Socket(s):           1\n",
        "Core(s) per socket:  1\n",
        "Thread(s) per core:  2\n",
        "L3 cache:            56320K\n",
        "CPU MHz:             2200.000\n",
        "13G\n",
        "Avail\n",
        "34G\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWPQqb3oETLd",
        "outputId": "f96ea169-c2f0-4658-aac7-703637ceafcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#GPU count and name\n",
        "!nvidia-smi -L\n",
        "\n",
        "#use this command to see GPU activity while doing Deep Learning tasks, for this command 'nvidia-smi' and for above one to work, go to 'Runtime > change runtime type > Hardware Accelerator > GPU'\n",
        "!nvidia-smi\n",
        "\n",
        "!lscpu |grep 'Model name'\n",
        "\n",
        "#no.of sockets i.e available slots for physical processors\n",
        "!lscpu | grep 'Socket(s):'\n",
        "\n",
        "#no.of cores each processor is having \n",
        "!lscpu | grep 'Core(s) per socket:'\n",
        "\n",
        "#no.of threads each core is having\n",
        "!lscpu | grep 'Thread(s) per core'\n",
        "\n",
        "!lscpu | grep \"L3 cache\"\n",
        "\n",
        "#if it had turbo boost it would've shown Min and Max MHz also but it is only showing current frequency this means it always operates at 2.3GHz\n",
        "!lscpu | grep \"MHz\"\n",
        "\n",
        "#memory that we can use\n",
        "!free -h --si | awk  '/Mem:/{print $2}'\n",
        "\n",
        "#hard disk space that we can use\n",
        "!df -h / | awk '{print $4}'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-fbc6f829-4182-d9a5-e62e-dfafbf4e919b)\n",
            "Mon Nov 16 19:20:54 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0    27W /  70W |    227MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "Socket(s):           1\n",
            "Core(s) per socket:  1\n",
            "Thread(s) per core:  2\n",
            "L3 cache:            56320K\n",
            "CPU MHz:             2200.000\n",
            "13G\n",
            "Avail\n",
            "38G\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUA8ZfUsGIVs"
      },
      "source": [
        "# IMPORTANT: For every 12hrs or so Disk, RAM, VRAM, CPU cache etc data that is on our alloted virtual machine will get erased. MAKE SURE TO SAVE YOUR DATA.\n",
        "\n",
        "The following code cell runs some code for Tensorflow-gpu to ensure that it can access the colab GPU. The output should contain something similar to:\n",
        "\n",
        "```\n",
        "physical_device_desc: \"device: XLA_GPU device\"\n",
        ", name: \"/device:GPU:0\"\n",
        "device_type: \"GPU\"\n",
        "memory_limit: 15701463552\n",
        "locality {\n",
        "  bus_id: 1\n",
        "  links {\n",
        "  }\n",
        "}\n",
        "incarnation: 12081463421592476599\n",
        "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
        "]\n",
        "``` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hi23nWbEy6V",
        "outputId": "249d4ba3-a537-4afe-fdbf-7d6448afab50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow.python.client import device_lib \n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 15803943982971643543\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 13933488675974232623\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 9884692621395071223\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14640891840\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 10838393606904985757\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCUuM_bQHMZt"
      },
      "source": [
        "The following command can be used as a bash command to monitor the usage and memory of your GPU.\n",
        "\n",
        "```\n",
        "!nvidia-smi\n",
        "```\n",
        "\n",
        "If you would like to monitor constant updates of the GPU, you can run the following for 1 second updates\n",
        "\n",
        "```\n",
        "!watch -n 1 nvidia-smi\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfPzGUoyHiLl",
        "outputId": "0419f813-cf0d-4c82-9aad-ac313a1100ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Nov 16 19:21:11 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0    28W /  70W |    227MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTa1iQlnHkOb"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}
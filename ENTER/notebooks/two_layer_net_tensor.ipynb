{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "two_layer_net_tensor.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tombresee/PyTorch/blob/main/ENTER/notebooks/two_layer_net_tensor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqd8aDDlLNzK"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj65RHQFLNzN"
      },
      "source": [
        "\n",
        "PyTorch: Tensors\n",
        "----------------\n",
        "\n",
        "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
        "predict y from x by minimizing squared Euclidean distance.\n",
        "\n",
        "This implementation uses PyTorch tensors to manually compute the forward pass,\n",
        "loss, and backward pass.\n",
        "\n",
        "A PyTorch Tensor is basically the same as a numpy array: it does not know\n",
        "anything about deep learning or computational graphs or gradients, and is just\n",
        "a generic n-dimensional array to be used for arbitrary numeric computation.\n",
        "\n",
        "The biggest difference between a numpy array and a PyTorch Tensor is that\n",
        "a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU,\n",
        "just cast the Tensor to a cuda datatype.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-G0gHo_LNzN",
        "outputId": "665739a6-0008-426c-bf4c-d64a94798ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "device = torch.device(\"cpu\")  # or can run directly on GPU \n",
        "\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
        "\n",
        "print('\\nx Tensor:\\n', x, '\\n')\n",
        "\n",
        "\n",
        "learning_rate = 1e-6\n",
        "\n",
        "\n",
        "for t in range(500):\n",
        "\n",
        "    # Forward pass: compute predicted y\n",
        "\n",
        "    h = x.mm(w1)\n",
        "\n",
        "    h_relu = h.clamp(min=0)\n",
        "\n",
        "    y_pred = h_relu.mm(w2)\n",
        "\n",
        "\n",
        "\n",
        "    # Compute and print loss\n",
        "\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "\n",
        "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "\n",
        "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "\n",
        "    grad_h = grad_h_relu.clone()\n",
        "\n",
        "    grad_h[h < 0] = 0\n",
        "\n",
        "    grad_w1 = x.t().mm(grad_h)\n",
        "\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "x Tensor:\n",
            " tensor([[-1.6825,  0.1330,  2.7445,  ...,  0.7654,  0.9186,  0.0717],\n",
            "        [-0.5132, -0.0851,  0.6023,  ..., -0.4918,  0.7064,  0.9295],\n",
            "        [ 1.0271, -2.0395, -0.1787,  ..., -0.7207, -0.2277, -1.2842],\n",
            "        ...,\n",
            "        [-0.3524,  0.7805,  1.8405,  ...,  1.2590, -1.8116,  0.5003],\n",
            "        [-0.1700,  1.6614,  0.7387,  ...,  0.2901, -1.3311, -0.5925],\n",
            "        [-0.9852, -1.3713,  0.8099,  ...,  0.8014, -0.0251,  1.4281]]) \n",
            "\n",
            "99 527.7888793945312\n",
            "199 3.076230764389038\n",
            "299 0.0364333912730217\n",
            "399 0.0008805262623354793\n",
            "499 0.00011173448001500219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt8939y9Lr8i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}